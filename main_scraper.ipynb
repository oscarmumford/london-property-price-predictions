{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036cc113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2f8184f6",
   "metadata": {},
   "source": [
    "x = ZooplaPageDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c907b60-aeab-4c48-963d-3d5eb7412467",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.scrape(\"https://www.zoopla.co.uk/for-sale/details/61532908/?search_identifier=8e874acb74a4f59f031ebea765f71b5a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cc699-c1f2-4b99-ad57-d37bca137cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.quit_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e100fa-4bb0-4868-ac08-bf7608391076",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.lat_long_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd07ce-2864-4503-9748-86bf57f7e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dab9af-fbb5-49e0-a220-00a8f665d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.scrape(\"https://www.zoopla.co.uk/for-sale/details/48598857/?search_identifier=8e874acb74a4f59f031ebea765f71b5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4a7e6-56af-4921-aeea-40b74a61ce25",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1995eb07-a61f-44f4-a3c3-1a0a3926104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#web scraping\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#plotting, data analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#Dealing with CSVs from scrape\n",
    "import csv\n",
    "import subprocess\n",
    "import os\n",
    "from os.path import exists, isfile, join\n",
    "from os import listdir\n",
    "import glob\n",
    "\n",
    "#scraping dependencies\n",
    "import time \n",
    "from time import sleep\n",
    "import re\n",
    "import random\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "from collections import defaultdict\n",
    "\n",
    "#my selenium model dependencies\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import selenium_code.selenium_scraper\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import selenium.webdriver.safari\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#import my selenium module\n",
    "from selenium_code.selenium_scraper import ZooplaPageDriver, USER_AGENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a857ea-fe68-4de7-9ffa-e97fe4918295",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SCRAPING VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe6b70-fd14-47a7-b31d-69a41411c7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "339db770-7025-489f-9569-dbfc21298da9",
   "metadata": {},
   "source": [
    "## SCRAPING FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd1d06-5e59-4757-a567-ab7bc9f93dd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### VPN Switcher Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff7a71c-16b7-4db9-8fd3-41782abc0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_2_random_vpn():\n",
    "    \"\"\"Connect to a random new vpn\"\"\"\n",
    "    print('There was an issue with scraping', '\\n\\n', 'Switching VPN location...','\\n')\n",
    "    with subprocess.Popen([\"ivpn\", \"servers\"], stdout=subprocess.PIPE) as proc:\n",
    "        #print(proc.stdout.read().decode())\n",
    "        vpns_stdouts = proc.stdout.read().decode()\n",
    "    vpns = [vpn_name if index % 5 == 1 else None for index, vpn_name in enumerate(vpns_stdouts.split('|')) if index > 5]\n",
    "    vpns = [item.strip() for item in vpns if item != None]\n",
    "    selected_vpn = str(random.choice(vpns))\n",
    "    with subprocess.Popen([\"ivpn\", \"connect\", selected_vpn], stdout=subprocess.PIPE) as proc:\n",
    "        print(proc.stdout.read().decode())\n",
    "    for wait_time in range(2,0,-1):\n",
    "        print(f'Wait: {wait_time} seconds remaining')\n",
    "        sleep(1)\n",
    "    print('0 seconds remaining')\n",
    "    print(f'\\nContinuing with scrape...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daacf647-c9a4-48bb-8811-c8a6f437336d",
   "metadata": {},
   "source": [
    "### Extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "161aba3c-a955-4a80-b316-0cab6f4aed32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an issue with scraping \n",
      "\n",
      " Switching VPN location... \n",
      "\n",
      "[WireGuard] Connecting to: Holon, Tel Aviv, IL (Israel) il.wg.ivpn.net UDP:2049...\n",
      "Connecting...\n",
      "VPN                    : CONNECTED\n",
      "                         il.wg.ivpn.net, Holon, Tel Aviv (IL), Israel\n",
      "    Protocol           : WireGuard\n",
      "    Local IP           : 172.17.63.113\n",
      "    Server IP          : 185.191.207.197\n",
      "    Connected          : 2022-06-04 17:44:55 +0100 BST\n",
      "DNS                    : Default (auto)\n",
      "Firewall               : Enabled\n",
      "    Allow LAN          : false\n",
      "    Allow IVPN servers : true\n",
      "\n",
      "Tips:\n",
      " ivpn disconnect         Stop current VPN connection\n",
      "\n",
      "\n",
      "Wait: 2 seconds remaining\n",
      "Wait: 1 seconds remaining\n",
      "0 seconds remaining\n",
      "\n",
      "Continuing with scrape...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81528c26-caf7-4aaf-98dd-7e0a5fa1bc5a",
   "metadata": {},
   "source": [
    "### NEED TO COOPT THIS CODE TO SCRAPE THE ZOOPLA PAGE!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0866544c-7acd-4203-b45e-9d84149901ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_zoopla_url(page_index, search_area):\n",
    "    \"\"\"create the URL for the next page\"\"\"\n",
    "    return f\"https://www.zoopla.co.uk/for-sale/property/london/?q={search_area}&results_sort=newest_listings&search_source=home&pn={page_index}\"\n",
    "\n",
    "\n",
    "def make_soup(url, headers):\n",
    "    \"\"\"make soup from passed URL\"\"\"\n",
    "    r = requests.get(url, headers)\n",
    "    return BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "def get_total_pages_for_query(soup, results_per_page):\n",
    "    \"\"\"identify number of pages to search through\"\"\"\n",
    "    results_count_str = soup.find('p', attrs={'data-testid':\"total-results\"}).text\n",
    "    listings_str = re.match(r\"[0-9]+\", results_count_str).group(0)\n",
    "    listings = int(listings_str)\n",
    "    return ceil(listings / results_per_page)\n",
    "\n",
    "\n",
    "def zoopla_search_extractor(soup):\n",
    "    \"\"\"extract properties\"\"\"\n",
    "    \n",
    "    extraction_dict = {'address':[],\n",
    "                       'price':[],\n",
    "                       'bedrooms':[],\n",
    "                       'bathrooms':[],\n",
    "                       'lounges':[],\n",
    "                       'url':[],\n",
    "                       'nearby_station_1':[],\n",
    "                       'nearby_station_2':[]\n",
    "                      }\n",
    "    \n",
    "    \n",
    "    property_listings = soup.find_all('div', attrs={\"data-testid\":re.compile(r\"search-result_listing_\")})\n",
    "    \n",
    "    for property in property_listings:\n",
    "        \n",
    "        \n",
    "        #location\n",
    "        try:\n",
    "            address = property.find('p', attrs={\"data-testid\":\"listing-description\"}).text\n",
    "        except:\n",
    "            address = None\n",
    "        extraction_dict['address'].append(address)\n",
    "\n",
    "        \n",
    "        #price\n",
    "        try:\n",
    "            price = property.find('div', attrs={\"data-testid\":re.compile(r\"listing-price\")} ).text\n",
    "        except:\n",
    "            price = None\n",
    "        extraction_dict['price'].append(price)\n",
    "        \n",
    "        \n",
    "        #bedrooms\n",
    "        try:\n",
    "            bedrooms = property.find('span', attrs={\"data-testid\":\"bed\"}).parent.next_sibling.text\n",
    "        except:\n",
    "            bedrooms = None\n",
    "        extraction_dict['bedrooms'].append(bedrooms)\n",
    "        \n",
    "        #bathrooms        \n",
    "        try:\n",
    "            bathrooms = property.find('span', attrs={\"data-testid\":\"bath\"}).parent.next_sibling.text\n",
    "        except:\n",
    "            bathrooms = None\n",
    "        extraction_dict['bathrooms'].append(bathrooms)\n",
    "        \n",
    "        #lounges\n",
    "        try:\n",
    "            lounges = property.find('span', attrs={\"data-testid\":\"chair\"}).parent.next_sibling.text\n",
    "        except:\n",
    "            lounges = None    \n",
    "            \n",
    "        extraction_dict['lounges'].append(lounges)\n",
    "        \n",
    "        #URL to listing page\n",
    "        try:\n",
    "            property_url = property.find('a', attrs={\"data-testid\":\"listing-details-link\"})['href']\n",
    "            property_url_available = True\n",
    "        except:\n",
    "            property_url = None\n",
    "            property_url_available = False\n",
    "        extraction_dict['url'].append(property_url)\n",
    "        \n",
    "        #NEARBY LOCATION DATA\n",
    "        try:\n",
    "            nearby_locations = property.find('div', attrs={\"data-testid\":\"listing-transport\"})\n",
    "            nearby_locations = [location.text for location in nearby_locations.children]\n",
    "            \n",
    "            try:\n",
    "                extraction_dict['nearby_station_1'].append(nearby_locations[0])\n",
    "            except:\n",
    "                extraction_dict['nearby_station_1'].append(None)\n",
    "            try:\n",
    "                extraction_dict['nearby_station_2'].append(nearby_locations[1])\n",
    "            except:\n",
    "                extraction_dict['nearby_station_2'].append(None)\n",
    "        except:\n",
    "            extraction_dict['nearby_station_1'].append(None)\n",
    "            extraction_dict['nearby_station_2'].append(None)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return extraction_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00105e70-97de-4f8e-8c39-94ceed8c3c20",
   "metadata": {},
   "source": [
    "### Page Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f50c735d-651d-4d42-91fa-cb180595324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoopla_all_listings_scraper(location, n_pages=1, delay=2):\n",
    "    \n",
    "    #Creating the dictionary which will be used to save our scraped data on each iteration before saving in .csv\n",
    "    extraction = {'address':[],\n",
    "                       'price':[],\n",
    "                       'bedrooms':[],\n",
    "                       'bathrooms':[],\n",
    "                       'lounges':[],\n",
    "                       'url':[],\n",
    "                       'nearby_station_1':[],\n",
    "                       'nearby_station_2':[]}\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #creating .csv files if they do not already exist. They will be appended to during the scrape\n",
    "    #creating file\n",
    "    file_location = \"data/csv_data/\" + \"_\".join(location.split()) + \"_scrape_.csv\"\n",
    "    keys = extraction.keys()\n",
    "    if not exists(file_location):\n",
    "        with open(file_location, \"w\") as outfile:\n",
    "            writer = csv.writer(outfile, delimiter = \"\\t\")\n",
    "            writer.writerow(keys)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Formatting the query and location strings to be usable in our URL\n",
    "    # fmat_location= \"%20\".join(location.strip().split()).lower()    #place separated by %20\n",
    "    # fmat_query= \"%20\".join(query.strip().split()).lower()          #listing title separated by %20\n",
    "     \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    #if all pages are to be scraped then get the number of pages\n",
    "    # Keep trying by changing vpns until we have this result up to a limited maximum number of tries\n",
    "    results_per_page = 25\n",
    "    listing_num_scraped = False\n",
    "    vpn_switch_counter = 0\n",
    "    \n",
    "    if n_pages == 'all':\n",
    "        while not listing_num_scraped:\n",
    "            try:\n",
    "\n",
    "                url = make_zoopla_url(0, location)            \n",
    "                header_choice = {\"User-Agent\":random.choice(USER_AGENTS)}\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "                n_pages = get_total_pages_for_query(soup, results_per_page)\n",
    "                listing_num_scraped = True\n",
    "            except:\n",
    "                print(\"Couldn't scrape the number of listings\")\n",
    "                print(\"Attempting a different VPN\")\n",
    "                connect_2_random_vpn() #switches the VPN\n",
    "                vpn_switch_counter += 1\n",
    "                if vpn_switch_counter == 5:\n",
    "                    break\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # TIME TO SCRAPE EACH PAGE!\n",
    "    print(f\"Commencing Scrape of {n_pages} pages\")\n",
    "    print(f\"Scraping {n_pages * results_per_page} listings in {location.upper()}\")\n",
    "\n",
    "    print(\"!\"*50)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # SCRAPE FOR LOOP ####################################################################################\n",
    "    \n",
    "    total_captchas = 0\n",
    "    for current_page_index in range(1, n_pages+1):\n",
    "        \n",
    "        print(f'Attempting to scrape page {current_page_index} of {n_pages}')\n",
    "    \n",
    "        current_page_url = make_zoopla_url(current_page_index, location)\n",
    "        \n",
    "        #Using requests to return json and then turning to soup using our defined soupify function\n",
    "        \n",
    "        possible_captcha = True\n",
    "        captcha_counter = 0\n",
    "        while possible_captcha: \n",
    "            #Set a limit on the number of times we try to switch VPN\n",
    "            if captcha_counter == 5:\n",
    "                break\n",
    "            \n",
    "            #Connect to the page using random headeres \n",
    "            header_choice = {\"User-Agent\":random.choice(USER_AGENTS)}\n",
    "            r = requests.get(current_page_url, headers=header_choice)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            \n",
    "            #If we get CAPTCHA'd try a different VPN server and retry connecting. Increase captcha_counter by 1\n",
    "            #If NOT CAPTCHA'd then break the while loop and continue\n",
    "            #print(len(soup.get_text().lower()))\n",
    "            if 'captcha' in soup.get_text().lower(): \n",
    "                captcha_counter += 1\n",
    "                print(\"~o\" * 5 + \"CAPTCHA DETECTED!\" + \"o~\" *5)\n",
    "                connect_2_random_vpn()\n",
    "            else:\n",
    "                print('No CAPTCHA on page')\n",
    "                possible_captcha = False\n",
    "          \n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        # Attempt to extract all of the data from the page which we hope is not still CAPTCHA'd\n",
    "        #zoopla_search_extractor(soup)\n",
    "        for col_name, data in zoopla_search_extractor(soup).items():\n",
    "            extraction[col_name].extend(data)\n",
    "            print(f\"{col_name}: {len([points for points in data if points != None])}\")\n",
    "        # Add any more captchas to the TOTAL CAPTCHAS\n",
    "        total_captchas += captcha_counter\n",
    "        \n",
    "        # APPEND TO CSV any data extracted at certain intervals\n",
    "        \n",
    "        #when to save work to csv\n",
    "        if current_page_index % 5 == 0 or current_page_index == n_pages:\n",
    "            #Every n pages save the output\n",
    "            with open(file_location, \"a\") as outfile:\n",
    "                writer = csv.writer(outfile, delimiter = \"\\t\")\n",
    "                writer.writerows(zip(*[extraction[key] for key in keys]))\n",
    "            \n",
    "            #Empty the extraction variable for the next scrapes\n",
    "            print(extraction)\n",
    "            extraction = {key:[] for key in extraction.keys()}\n",
    "            \n",
    "            \n",
    "            \n",
    "        print(f\"Number of items currently in extraction buffer:{len(extraction['address'])}\")\n",
    "        print(f\"TOTAL CAPTCHAS DURING SCRAPE: {total_captchas}\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        #DELAY loading next page by preset amount\n",
    "        sleep(delay)\n",
    "        \n",
    "    return extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733098dd-b00a-48c9-b97f-c1f83d9bba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zoopla_all_listings_scraper(\"london\", n_pages='all', delay=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8356f171-73b6-43d0-83ac-afaab520809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_uri = \"./data/csv_data/london_scrape.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_uri, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0481a77f-59ef-42d7-9877-c3da1c71a392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>lounges</th>\n",
       "      <th>url</th>\n",
       "      <th>nearby_station_1</th>\n",
       "      <th>nearby_station_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Park Street, Chelsea/London SW6</td>\n",
       "      <td>£1,375,000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>/new-homes/details/61623573/?search_identifier...</td>\n",
       "      <td>0.1 miles Imperial Wharf</td>\n",
       "      <td>0.2 miles Chelsea Harbour Pier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bridges Court, London SW11</td>\n",
       "      <td>£440,000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/for-sale/details/61567435/?search_identifier=...</td>\n",
       "      <td>0.4 miles Chelsea Harbour Pier</td>\n",
       "      <td>0.4 miles Imperial Wharf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carshalton Grove, Sutton SM1</td>\n",
       "      <td>Guide price£650,000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>/for-sale/details/61623414/?search_identifier=...</td>\n",
       "      <td>0.6 miles Carshalton</td>\n",
       "      <td>0.6 miles Sutton (London)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rutland Rd, Forest Gate E7</td>\n",
       "      <td>Guide price£545,000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>/for-sale/details/61623411/?search_identifier=...</td>\n",
       "      <td>0.4 miles East Ham</td>\n",
       "      <td>0.5 miles Upton Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rutland Rd, Forest Gate E7</td>\n",
       "      <td>Guide price£545,000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>/for-sale/details/61623405/?search_identifier=...</td>\n",
       "      <td>0.4 miles East Ham</td>\n",
       "      <td>0.5 miles Upton Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Burlington Road, Osterley, Isleworth TW7</td>\n",
       "      <td>£750,000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>/for-sale/details/61476741/?search_identifier=...</td>\n",
       "      <td>0.3 miles Osterley</td>\n",
       "      <td>0.5 miles Hounslow East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Tufnell Park, Tufnell Park, London N7</td>\n",
       "      <td>£375,000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>/for-sale/details/61476757/?search_identifier=...</td>\n",
       "      <td>0.5 miles Upper Holloway</td>\n",
       "      <td>0.5 miles Tufnell Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Holmewood Gardens, Brixton Hill, London SW2</td>\n",
       "      <td>£700,000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>/for-sale/details/61476758/?search_identifier=...</td>\n",
       "      <td>0.6 miles Streatham Hill</td>\n",
       "      <td>0.8 miles Tulse Hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Coldershaw Road, Ealing W13</td>\n",
       "      <td>Guide price£535,000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>/for-sale/details/61476750/?search_identifier=...</td>\n",
       "      <td>0.5 miles West Ealing</td>\n",
       "      <td>0.6 miles Drayton Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Kilravock Street, London W10</td>\n",
       "      <td>£950,000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>/for-sale/details/61476731/?search_identifier=...</td>\n",
       "      <td>0.4 miles Queens Park (London)</td>\n",
       "      <td>0.4 miles Queen's Park</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          address                price  \\\n",
       "0                 Park Street, Chelsea/London SW6           £1,375,000   \n",
       "1                      Bridges Court, London SW11             £440,000   \n",
       "2                    Carshalton Grove, Sutton SM1  Guide price£650,000   \n",
       "3                      Rutland Rd, Forest Gate E7  Guide price£545,000   \n",
       "4                      Rutland Rd, Forest Gate E7  Guide price£545,000   \n",
       "...                                           ...                  ...   \n",
       "9995     Burlington Road, Osterley, Isleworth TW7             £750,000   \n",
       "9996        Tufnell Park, Tufnell Park, London N7             £375,000   \n",
       "9997  Holmewood Gardens, Brixton Hill, London SW2             £700,000   \n",
       "9998                  Coldershaw Road, Ealing W13  Guide price£535,000   \n",
       "9999                 Kilravock Street, London W10             £950,000   \n",
       "\n",
       "     bedrooms bathrooms lounges  \\\n",
       "0           2         2       1   \n",
       "1           1         1       1   \n",
       "2           3         2       2   \n",
       "3           3         1       2   \n",
       "4           3         1       2   \n",
       "...       ...       ...     ...   \n",
       "9995        3         1       2   \n",
       "9996        1         1      na   \n",
       "9997        2         1      na   \n",
       "9998        2         1       1   \n",
       "9999        3         1       2   \n",
       "\n",
       "                                                    url  \\\n",
       "0     /new-homes/details/61623573/?search_identifier...   \n",
       "1     /for-sale/details/61567435/?search_identifier=...   \n",
       "2     /for-sale/details/61623414/?search_identifier=...   \n",
       "3     /for-sale/details/61623411/?search_identifier=...   \n",
       "4     /for-sale/details/61623405/?search_identifier=...   \n",
       "...                                                 ...   \n",
       "9995  /for-sale/details/61476741/?search_identifier=...   \n",
       "9996  /for-sale/details/61476757/?search_identifier=...   \n",
       "9997  /for-sale/details/61476758/?search_identifier=...   \n",
       "9998  /for-sale/details/61476750/?search_identifier=...   \n",
       "9999  /for-sale/details/61476731/?search_identifier=...   \n",
       "\n",
       "                    nearby_station_1                nearby_station_2  \n",
       "0           0.1 miles Imperial Wharf  0.2 miles Chelsea Harbour Pier  \n",
       "1     0.4 miles Chelsea Harbour Pier        0.4 miles Imperial Wharf  \n",
       "2               0.6 miles Carshalton       0.6 miles Sutton (London)  \n",
       "3                 0.4 miles East Ham            0.5 miles Upton Park  \n",
       "4                 0.4 miles East Ham            0.5 miles Upton Park  \n",
       "...                              ...                             ...  \n",
       "9995              0.3 miles Osterley         0.5 miles Hounslow East  \n",
       "9996        0.5 miles Upper Holloway          0.5 miles Tufnell Park  \n",
       "9997        0.6 miles Streatham Hill            0.8 miles Tulse Hill  \n",
       "9998           0.5 miles West Ealing         0.6 miles Drayton Green  \n",
       "9999  0.4 miles Queens Park (London)          0.4 miles Queen's Park  \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b8e82-abee-4bd9-a8ac-eda05e1e8c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb122a-3a23-4207-9954-b67fd8ae9300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f454a11-c0ec-4858-92d3-5fec35773422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cda94e4-3113-4348-b426-8cd1b3cad6e3",
   "metadata": {},
   "source": [
    "### selenium extraction - Too slow for use in project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d39cd0-1105-40a0-9ac2-1c9926a59640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"selenium_extraction = {\n",
    "                        'lat':[],\n",
    "                       'long':[],\n",
    "                       'first_listed_date':[],\n",
    "                       'first_listed_text':[],\n",
    "                       'first_listed_price':[],\n",
    "                       'last_sold_date':[],\n",
    "                       'last_sold_text':[],\n",
    "                       'last_sold_price':[],\n",
    "                       'tenure':[],\n",
    "                       'council_tax_band':[]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tenure and council tax band to be added to selenium scrape\n",
    "        tenure = 'na'\n",
    "        extraction_dict['tenure'].append(tenure)\n",
    "        council_tax_band = 'na'\n",
    "        extraction_dict['council_tax_band'].append(council_tax_band)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #use Selenium class to scrape each listing\n",
    "        if property_url_available:\n",
    "            try:\n",
    "                scrape_url = \"https://www.zoopla.co.uk\" + property_url\n",
    "                selenium_scrape_instance = ZooplaPageDriver()\n",
    "                selenium_results = selenium_scrape_instance.scrape(scrape_url)\n",
    "                for key, value in selenium_results.items():\n",
    "                    extraction_dict[key].append(value)\n",
    "  \n",
    "                \n",
    "            except:\n",
    "                print('ERROR: Unable to scrape property url using selenium')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba68403-02e3-476b-b0d3-0c524a3dfe4d",
   "metadata": {},
   "source": [
    "### Second page scraper for zoopla historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5d440d-64d7-46f2-90fe-555db0c53bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_iter(soup, tagname):\n",
    "    tag = soup.find(tagname)\n",
    "    while tag is not None:\n",
    "        yield tag\n",
    "        tag = tag.find_next(tagname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c0852d-09bd-463f-9863-aa6191358992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_historical_zoopla_url(page_index, search_area):\n",
    "    \"\"\"create the URL for the next historical zoopla page\"\"\"\n",
    "    return f\"https://www.zoopla.co.uk/house-prices/london/?q={search_area}&search_source=house-prices&pn={page_index}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4edbefc0-f56e-41dd-acf6-439c09168176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def extract_historical_page_data(soup):\n",
    "\n",
    "        extract_columns = ['address', 'last_sold_date', 'last_sold_price', 'estimated_price', 'bedrooms', 'bathrooms', 'lounges', 'url', 'tenure', 'property_type' ]\n",
    "        extraction = {key:[] for key in extract_columns}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for li in soup.find('ul', attrs={'data-testid':'result-list'}):\n",
    "\n",
    "            each_property = li.div\n",
    "\n",
    "\n",
    "            try:\n",
    "                if each_property.attrs['aria-disabled'] == 'true':\n",
    "                    #property has been deactivated\n",
    "                    print('property has been deactivated')\n",
    "                    continue\n",
    "            except:\n",
    "                print('property missing aria-disabled attr')\n",
    "\n",
    "\n",
    "            try:\n",
    "                address = each_property.find('h2').text\n",
    "            except:\n",
    "                print('property missing h2 tag for address')\n",
    "\n",
    "\n",
    "            last_sold_date = None\n",
    "            last_sold_price = None\n",
    "            estimated_price = None\n",
    "            try:\n",
    "                price_data = each_property.findChild('div',recursive=False).findChildren('div', recursive=False)\n",
    "                for div in price_data:\n",
    "                    if 'Last sold' in div.text:\n",
    "                        last_sold_date = div.find('label').text\n",
    "                        last_sold_price = div.findChildren('label', recursive=False)[1].text\n",
    "                    elif 'Estimated price' in div.text:\n",
    "                            estimated_price = div.findChildren('label', recursive=False)[1].text\n",
    "            except:\n",
    "                print('failed to get price_data')\n",
    "\n",
    "\n",
    "            try:\n",
    "                property_url = each_property.a['href']\n",
    "            except:\n",
    "                print('failed to get property_url')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            bedrooms, bathrooms, lounges = None, None, None\n",
    "            tenure, property_type = None, None\n",
    "\n",
    "            #used for the first div in additional info (there could be 0, 1 or 2 divs)\n",
    "            try:\n",
    "                additional_info_divs = each_property.a.div.findChildren('div', recursive=False)[1].findChildren('div', recursive=False)\n",
    "                room_counts = additional_info_divs[0].children\n",
    "                for div in room_counts:\n",
    "                    if 'bed' in div.text.lower():\n",
    "                        bedrooms = re.sub(r'[^0-9]', '', div.text)\n",
    "                    elif 'bath' in div.text.lower():\n",
    "                        bathrooms = re.sub(r'\\D', '', div.text)\n",
    "                    elif 'reception' in div.text.lower():\n",
    "                        lounges = re.sub(r'\\D', '', div.text)\n",
    "                    else: #means there is only info on the tenure and property\n",
    "                        try:\n",
    "                            for each in div.findChildren('div', recursive=False):\n",
    "                                if 'freehold' in each.text.lower():\n",
    "                                    tenure = 'freehold'\n",
    "                                elif 'leasehold' in each.text.lower():\n",
    "                                    tenure = each.text.lower()\n",
    "                                else:\n",
    "                                    property_type = each.text.lower()\n",
    "                        except:\n",
    "                            print('failed to get tenure info')\n",
    "\n",
    "            except:\n",
    "                print('failed to get bed-bath-receiption info')\n",
    "\n",
    "\n",
    "            #for the case where there are two div's in additional info\n",
    "            try:\n",
    "                property_descriptions = additional_info_divs[1].findChildren('div', recursive=False)\n",
    "\n",
    "                for each in property_descriptions:\n",
    "\n",
    "                    if 'freehold' in each.text.lower():\n",
    "                        tenure = 'freehold'\n",
    "                    elif 'leasehold' in each.text.lower():\n",
    "                        tenure = each.text.lower()\n",
    "                    else:\n",
    "                        property_type = each.text.lower()\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            extraction['address'].append(address)\n",
    "            extraction['last_sold_date'].append(last_sold_date)\n",
    "            extraction['last_sold_price'].append(last_sold_price)\n",
    "            extraction['estimated_price'].append(estimated_price)\n",
    "            extraction['bedrooms'].append(bedrooms)\n",
    "            extraction['bathrooms'].append(bathrooms)\n",
    "            extraction['lounges'].append(lounges)\n",
    "            extraction['url'].append(property_url)\n",
    "            extraction['tenure'].append(tenure)\n",
    "            extraction['property_type'].append(property_type)\n",
    "\n",
    "        return extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bfe828-6aa3-4410-b7d7-1d0d6e72cf2f",
   "metadata": {},
   "source": [
    "### Historical crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddfed8-0f51-42c1-93af-3e98e2efed58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5bfbd4ed-d75b-4824-a337-e99c708d3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoopla_historical_listings_scraper(location, n_pages=1, delay=2):\n",
    "    \n",
    "    #Creating the dictionary which will be used to save our scraped data on each iteration before saving in .csv\n",
    "    extract_columns = ['address', 'last_sold_date', 'last_sold_price', 'estimated_price', 'bedrooms', 'bathrooms', 'lounges', 'url', 'tenure', 'property_type' ]\n",
    "    extraction = {key:[] for key in extract_columns}\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #creating .csv files if they do not already exist. They will be appended to during the scrape\n",
    "    #creating file\n",
    "    file_location = \"data/csv_data/\" + \"_\".join(location.split()) + \"_historical_scrape.csv\"\n",
    "    keys = extraction.keys()\n",
    "    if not exists(file_location):\n",
    "        with open(file_location, \"w\") as outfile:\n",
    "            writer = csv.writer(outfile, delimiter = \"\\t\")\n",
    "            writer.writerow(keys)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Formatting the query and location strings to be usable in our URL\n",
    "    # fmat_location= \"%20\".join(location.strip().split()).lower()    #place separated by %20\n",
    "    # fmat_query= \"%20\".join(query.strip().split()).lower()          #listing title separated by %20\n",
    "     \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    #if all pages are to be scraped then get the number of pages\n",
    "    # Keep trying by changing vpns until we have this result up to a limited maximum number of tries\n",
    "    results_per_page = 10\n",
    "    listing_num_scraped = False\n",
    "    vpn_switch_counter = 0\n",
    "    \n",
    "    if n_pages == 'all':\n",
    "        while not listing_num_scraped:\n",
    "            try:\n",
    "                \n",
    "                url = make_historical_zoopla_url(0, location)         \n",
    "                header_choice = {\"User-Agent\":random.choice(USER_AGENTS)}\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text, 'html.parser')\n",
    "                for i in find_iter(soup, 'label'):\n",
    "                    if 'properties found' in i.text:\n",
    "                        property_total_str = i.text.replace(\",\",\"\")\n",
    "                        print(property_total_str)\n",
    "                        historical_property_count = int(re.search(r\"[\\d]+\", property_total_str).group(0))\n",
    "                        print(historical_property_count)\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                n_pages = int(ceil(historical_property_count / results_per_page))\n",
    "                listing_num_scraped = True\n",
    "            except:\n",
    "                print(\"Couldn't scrape the number of listings\")\n",
    "                print(\"Attempting a different VPN\")\n",
    "                connect_2_random_vpn() #switches the VPN\n",
    "                vpn_switch_counter += 1\n",
    "                if vpn_switch_counter == 5:\n",
    "                    break\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # TIME TO SCRAPE EACH PAGE!\n",
    "    print(f\"Commencing Scrape of {n_pages} pages\")\n",
    "    print(f\"Scraping {historical_property_count} listings in {location.upper()}\")\n",
    "\n",
    "    print(\"!\"*50)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # SCRAPE FOR LOOP ####################################################################################\n",
    "    \n",
    "    total_captchas = 0\n",
    "    for current_page_index in range(1090, n_pages+1):\n",
    "        \n",
    "        print(f'Attempting to scrape page {current_page_index} of {n_pages}')\n",
    "    \n",
    "        current_page_url = make_historical_zoopla_url(current_page_index, location)\n",
    "        \n",
    "        #Using requests to return json and then turning to soup using our defined soupify function\n",
    "        \n",
    "        possible_captcha = True\n",
    "        captcha_counter = 0\n",
    "        while possible_captcha: \n",
    "            #Set a limit on the number of times we try to switch VPN\n",
    "            if captcha_counter == 5:\n",
    "                break\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                #Connect to the page using random headeres \n",
    "                header_choice = {\"User-Agent\":random.choice(USER_AGENTS)}\n",
    "                r = requests.get(current_page_url, headers=header_choice)\n",
    "            except:\n",
    "                sleep(20)\n",
    "                connect_2_random_vpn()\n",
    "                captcha_counter += 1\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            \n",
    "            #If we get CAPTCHA'd try a different VPN server and retry connecting. Increase captcha_counter by 1\n",
    "            #If NOT CAPTCHA'd then break the while loop and continue\n",
    "            #print(len(soup.get_text().lower()))\n",
    "            \n",
    "            for h1 in soup.find_all('h1'):\n",
    "                if h1.text == \"House prices in London\":\n",
    "                    possible_captcha = False\n",
    "                    print('No CAPTCHA on page')\n",
    "                    break\n",
    "            if not possible_captcha:\n",
    "                break\n",
    "            print(\"~o\" * 5 + \"CAPTCHA DETECTED!\" + \"o~\" *5)\n",
    "            captcha_counter += 1\n",
    "            connect_2_random_vpn()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "          \n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        # Attempt to extract all of the data from the page which we hope is not still CAPTCHA'd\n",
    "        for col_name, data in extract_historical_page_data(soup).items():\n",
    "            extraction[col_name].extend(data)\n",
    "            print(f\"{col_name}: {len([points for points in data if points != None])}\")\n",
    "        # Add any more captchas to the TOTAL CAPTCHAS\n",
    "        total_captchas += captcha_counter\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Number of items currently in extraction buffer:{len(extraction['address'])}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # APPEND TO CSV any data extracted at certain intervals\n",
    "        \n",
    "        #when to save work to csv\n",
    "        if current_page_index % 5 == 0 or current_page_index == n_pages:\n",
    "            #Every n pages save the output\n",
    "            with open(file_location, \"a\") as outfile:\n",
    "                writer = csv.writer(outfile, delimiter = \"\\t\")\n",
    "                writer.writerows(zip(*[extraction[key] for key in keys]))\n",
    "            \n",
    "            #Empty the extraction variable for the next scrapes\n",
    "            \n",
    "            extraction = {key:[] for key in extraction.keys()}\n",
    "            print(\"Emptying buffer...number of addresses now in buffer:\", len(extraction['address']))\n",
    "                 \n",
    "        \n",
    "        print(f\"TOTAL CAPTCHAS DURING SCRAPE: {total_captchas}\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        #DELAY loading next page by preset amount\n",
    "        sleep(delay)\n",
    "        \n",
    "    return extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d79c5-93a5-44c3-aa93-684fcc687d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zoopla_historical_listings_scraper(\"london\", n_pages=\"all\", delay=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697315d-3573-4414-af68-6e4e410a5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_uri = \"./data/csv_data/london_historical_scrape.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(historical_data_uri, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2375b-3773-46f9-81f5-40222be89681",
   "metadata": {},
   "source": [
    "### Repurposing the historical scraper to randomly select pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bb2ab-ef39-4984-b920-806b53fe6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoopla_random_historical_listings_scraper(location, n_pages=1, delay=2):\n",
    "    \n",
    "    #Creating the dictionary which will be used to save our scraped data on each iteration before saving in .csv\n",
    "    extract_columns = ['address', 'last_sold_date', 'last_sold_price', 'estimated_price', 'bedrooms', 'bathrooms', 'lounges', 'url', 'tenure', 'property_type' ]\n",
    "    extraction = {key:[] for key in extract_columns}\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #creating .csv files if they do not already exist. They will be appended to during the scrape\n",
    "    #creating file\n",
    "    file_location = \"data/csv_data/\" + \"_\".join(location.split()) + \"_random_historical_scrape.csv\"\n",
    "    keys = extraction.keys()\n",
    "    if not exists(file_location):\n",
    "        with open(file_location, \"w\") as outfile:\n",
    "            writer = csv.writer(outfile, delimiter = \"\\t\")\n",
    "            writer.writerow(keys)\n",
    "     \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    #if all pages are to be scraped then get the number of pages\n",
    "    # Keep trying by changing vpns until we have this result up to a limited maximum number of tries\n",
    "    results_per_page = 10\n",
    "    listing_num_scraped = False\n",
    "    vpn_switch_counter = 0\n",
    "    \n",
    "    if n_pages == 'all':\n",
    "        while not listing_num_scraped:\n",
    "            try:\n",
    "                \n",
    "                url = make_historical_zoopla_url(0, location)         \n",
    "                header_choice = {\"User-Agent\":random.choice(USER_AGENTS)}\n",
    "                r = requests.get(url)\n",
    "                soup = BeautifulSoup(r.text, 'html.parser')\n",
    "                for i in find_iter(soup, 'label'):\n",
    "                    if 'properties found' in i.text:\n",
    "                        property_total_str = i.text.replace(\",\",\"\")\n",
    "                        print(property_total_str)\n",
    "                        historical_property_count = int(re.search(r\"[\\d]+\", property_total_str).group(0))\n",
    "                        print(historical_property_count)\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                n_pages = int(ceil(historical_property_count / results_per_page))\n",
    "                listing_num_scraped = True\n",
    "            except:\n",
    "                print(\"Couldn't scrape the number of listings\")\n",
    "                print(\"Attempting a different VPN\")\n",
    "                connect_2_random_vpn() #switches the VPN\n",
    "                vpn_switch_counter += 1\n",
    "                if vpn_switch_counter == 5:\n",
    "                    break\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # TIME TO SCRAPE EACH PAGE!\n",
    "    print(f\"Commencing Scrape of {n_pages} pages\")\n",
    "    print(f\"Scraping {historical_property_count} listings in {location.upper()}\")\n",
    "\n",
    "    print(\"!\"*50)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # SCRAPE FOR LOOP ####################################################################################\n",
    "    \n",
    "    total_captchas = 0\n",
    "    for current_page_index in random.sample(range(1090, n_pages+1):\n",
    "        \n",
    "        print(f'Attempting to scrape page {current_page_index} of {n_pages}')\n",
    "    \n",
    "        current_page_url = make_historical_zoopla_url(current_page_index, location)\n",
    "        \n",
    "        #Using requests to return json and then turning to soup using our defined soupify function\n",
    "        \n",
    "        possible_captcha = True\n",
    "        captcha_counter = 0\n",
    "        while possible_captcha: \n",
    "            #Set a limit on the number of times we try to switch VPN\n",
    "            if captcha_counter == 5:\n",
    "                break\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                #Connect to the page using random headeres \n",
    "                header_choice = {\"User-Agent\":random.choice(USER_AGENTS)}\n",
    "                r = requests.get(current_page_url, headers=header_choice)\n",
    "            except:\n",
    "                sleep(20)\n",
    "                connect_2_random_vpn()\n",
    "                captcha_counter += 1\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            \n",
    "            #If we get CAPTCHA'd try a different VPN server and retry connecting. Increase captcha_counter by 1\n",
    "            #If NOT CAPTCHA'd then break the while loop and continue\n",
    "            #print(len(soup.get_text().lower()))\n",
    "            \n",
    "            for h1 in soup.find_all('h1'):\n",
    "                if h1.text == \"House prices in London\":\n",
    "                    possible_captcha = False\n",
    "                    print('No CAPTCHA on page')\n",
    "                    break\n",
    "            if not possible_captcha:\n",
    "                break\n",
    "            print(\"~o\" * 5 + \"CAPTCHA DETECTED!\" + \"o~\" *5)\n",
    "            captcha_counter += 1\n",
    "            connect_2_random_vpn()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "          \n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        # Attempt to extract all of the data from the page which we hope is not still CAPTCHA'd\n",
    "        for col_name, data in extract_historical_page_data(soup).items():\n",
    "            extraction[col_name].extend(data)\n",
    "            print(f\"{col_name}: {len([points for points in data if points != None])}\")\n",
    "        # Add any more captchas to the TOTAL CAPTCHAS\n",
    "        total_captchas += captcha_counter\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\"Number of items currently in extraction buffer:{len(extraction['address'])}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # APPEND TO CSV any data extracted at certain intervals\n",
    "        \n",
    "        #when to save work to csv\n",
    "        if current_page_index % 5 == 0 or current_page_index == n_pages:\n",
    "            #Every n pages save the output\n",
    "            with open(file_location, \"a\") as outfile:\n",
    "                writer = csv.writer(outfile, delimiter = \"\\t\")\n",
    "                writer.writerows(zip(*[extraction[key] for key in keys]))\n",
    "            \n",
    "            #Empty the extraction variable for the next scrapes\n",
    "            \n",
    "            extraction = {key:[] for key in extraction.keys()}\n",
    "            print(\"Emptying buffer...number of addresses now in buffer:\", len(extraction['address']))\n",
    "                 \n",
    "        \n",
    "        print(f\"TOTAL CAPTCHAS DURING SCRAPE: {total_captchas}\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        #DELAY loading next page by preset amount\n",
    "        sleep(delay)\n",
    "        \n",
    "    return extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b73effb-1614-4ed2-b728-a72362826fec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q6/lwrvjh5x78jbrc_15wzqk_hc0000gp/T/ipykernel_16907/488979949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "random.sample(range(1000000), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bb726-a1f3-4e75-8088-5c31977931c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec4c5741-89c5-4929-a609-0bf359bf56d1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>last_sold_date</th>\n",
       "      <th>last_sold_price</th>\n",
       "      <th>estimated_price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>lounges</th>\n",
       "      <th>url</th>\n",
       "      <th>tenure</th>\n",
       "      <th>property_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Bickley Crescent, Bromley, BR1 2DN</td>\n",
       "      <td>Last sold  - Oct 1999</td>\n",
       "      <td>£97,500</td>\n",
       "      <td>£461,000 - £564,000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/100020390465/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>semi-detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 Chadd Drive, Bromley, BR1 2DP</td>\n",
       "      <td>Last sold  - Jul 2018</td>\n",
       "      <td>£675,000</td>\n",
       "      <td>£789,000 - £872,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020393770/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 Page Heath Lane, Bromley, BR1 2DR</td>\n",
       "      <td>Last sold  - Jan 2007</td>\n",
       "      <td>£480,000</td>\n",
       "      <td>£740,000 - £905,000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/100020408794/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>semi-detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Beresford Drive, Bromley, BR1 2DU</td>\n",
       "      <td>Last sold  - Nov 2006</td>\n",
       "      <td>£420,000</td>\n",
       "      <td>£830,000 - £918,000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>/property/uprn/100020390386/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 St Michael's Close, Bromley, BR1 2DX</td>\n",
       "      <td>Last sold  - Nov 2006</td>\n",
       "      <td>£376,000</td>\n",
       "      <td>£740,000 - £904,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020416127/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1 Southborough Road, Bromley, BR1 2EA</td>\n",
       "      <td>Last sold  - May 1998</td>\n",
       "      <td>£185,000</td>\n",
       "      <td>£973,000 - £1,189,000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/100020415051/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1 Lancaster Gardens, Bromley, BR1 2ED</td>\n",
       "      <td>Last sold  - Feb 2002</td>\n",
       "      <td>£375,000</td>\n",
       "      <td>£588,000 - £882,000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/10003617154/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>mid terrace house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1 Falcon Avenue, Bromley, BR1 2EH</td>\n",
       "      <td>Last sold  - Oct 2009</td>\n",
       "      <td>£571,000</td>\n",
       "      <td>£1,049,000 - £1,159,000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/100020397356/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1 Parkside Avenue, Bromley, BR1 2EJ</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£506,000 - £619,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020409903/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1 Sunningdale Road, Bromley, BR1 2EU</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£718,000 - £878,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020416607/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1 Burford Road, Bromley, BR1 2EY</td>\n",
       "      <td>Last sold  - Aug 1998</td>\n",
       "      <td>£219,000</td>\n",
       "      <td>£827,000 - £1,011,000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/100020392820/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1 Baxter Close, Bromley, BR1 2FA</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£522,000 - £638,000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/property/uprn/10070000055/</td>\n",
       "      <td>leasehold</td>\n",
       "      <td>end terrace house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1 Belfry Close, Bromley, BR1 2FB</td>\n",
       "      <td>Last sold  - Dec 2006</td>\n",
       "      <td>£725,000</td>\n",
       "      <td>£1,251,000 - £1,529,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/10070000046/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1 Daly Drive, Bromley, BR1 2FF</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£509,000 - £622,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/10070000090/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>semi-detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1 Fidgeon Close, Bromley, BR1 2FG</td>\n",
       "      <td>Last sold  - Aug 2017</td>\n",
       "      <td>£1,135,000</td>\n",
       "      <td>£1,255,000 - £1,387,000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/10070000113/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1 Gardenia Road, Bromley, BR1 2FH</td>\n",
       "      <td>Last sold  - Mar 2019</td>\n",
       "      <td>£325,000</td>\n",
       "      <td>£363,000 - £401,000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/property/uprn/10070000302/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>flat/maisonette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1 Parkland Mead, Bromley, BR1 2FQ</td>\n",
       "      <td>Last sold  - Mar 2014</td>\n",
       "      <td>£485,000</td>\n",
       "      <td>£765,000 - £845,000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/10070000497/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>end terrace house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1 Parnham Close, Bromley, BR1 2FR</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£640,000 - £960,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/10070000521/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>end terrace house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1 Percy Terrace, Bromley, BR1 2FS</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£454,000 - £681,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/10070000553/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>end terrace house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1 Sanderson Square, Bromley, BR1 2FT</td>\n",
       "      <td>Last sold  - Dec 2007</td>\n",
       "      <td>£745,000</td>\n",
       "      <td>£1,254,000 - £1,386,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/10070000569/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1 Cavendish Place, Bromley, BR1 2GA</td>\n",
       "      <td>Last sold  - Mar 2006</td>\n",
       "      <td>£850,000</td>\n",
       "      <td>£1,683,000 - £1,860,000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/10013152895/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Carrington, 1 Timms Close, Bromley, BR1 2GZ</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£1,478,000 - £2,217,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/10013152686/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1 Mayfield Road, Bromley, BR1 2HB</td>\n",
       "      <td>Last sold  - Jun 2016</td>\n",
       "      <td>£720,000</td>\n",
       "      <td>£849,000 - £1,038,000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>/property/uprn/100020406091/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1 Lindsey Close, Bromley, BR1 2HE</td>\n",
       "      <td>Last sold  - Mar 2017</td>\n",
       "      <td>£496,995</td>\n",
       "      <td>£573,000 - £633,000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020404698/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>end terrace house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1 Birdham Close, Bromley, BR1 2HF</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£706,000 - £781,000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>/property/uprn/100020390771/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>semi-detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1 Hawthorne Road, Bromley, BR1 2HG</td>\n",
       "      <td>Last sold  - Jul 2010</td>\n",
       "      <td>£1,425,000</td>\n",
       "      <td>£1,469,000 - £1,795,000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>/property/uprn/100020399817/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1 Hawthorne Close, Bromley, BR1 2HJ</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£1,183,000 - £1,775,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020399810/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1 Coombe Lea, Bromley, BR1 2HQ</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£1,003,000 - £1,504,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020395001/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>detached house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1 Barfield Road, Bromley, BR1 2HR</td>\n",
       "      <td>Last sold</td>\n",
       "      <td>-</td>\n",
       "      <td>£887,000 - £980,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020389621/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>detached bungalow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1 Wellsmoor Gardens, Bromley, BR1 2HT</td>\n",
       "      <td>Last sold  - Apr 1997</td>\n",
       "      <td>£82,500</td>\n",
       "      <td>£515,000 - £629,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/property/uprn/100020418834/</td>\n",
       "      <td>freehold</td>\n",
       "      <td>semi-detached property</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        address         last_sold_date  \\\n",
       "0          1 Bickley Crescent, Bromley, BR1 2DN  Last sold  - Oct 1999   \n",
       "1               1 Chadd Drive, Bromley, BR1 2DP  Last sold  - Jul 2018   \n",
       "2           1 Page Heath Lane, Bromley, BR1 2DR  Last sold  - Jan 2007   \n",
       "3           1 Beresford Drive, Bromley, BR1 2DU  Last sold  - Nov 2006   \n",
       "4        1 St Michael's Close, Bromley, BR1 2DX  Last sold  - Nov 2006   \n",
       "5         1 Southborough Road, Bromley, BR1 2EA  Last sold  - May 1998   \n",
       "6         1 Lancaster Gardens, Bromley, BR1 2ED  Last sold  - Feb 2002   \n",
       "7             1 Falcon Avenue, Bromley, BR1 2EH  Last sold  - Oct 2009   \n",
       "8           1 Parkside Avenue, Bromley, BR1 2EJ             Last sold    \n",
       "9          1 Sunningdale Road, Bromley, BR1 2EU             Last sold    \n",
       "10             1 Burford Road, Bromley, BR1 2EY  Last sold  - Aug 1998   \n",
       "11             1 Baxter Close, Bromley, BR1 2FA             Last sold    \n",
       "12             1 Belfry Close, Bromley, BR1 2FB  Last sold  - Dec 2006   \n",
       "13               1 Daly Drive, Bromley, BR1 2FF             Last sold    \n",
       "14            1 Fidgeon Close, Bromley, BR1 2FG  Last sold  - Aug 2017   \n",
       "15            1 Gardenia Road, Bromley, BR1 2FH  Last sold  - Mar 2019   \n",
       "16            1 Parkland Mead, Bromley, BR1 2FQ  Last sold  - Mar 2014   \n",
       "17            1 Parnham Close, Bromley, BR1 2FR             Last sold    \n",
       "18            1 Percy Terrace, Bromley, BR1 2FS             Last sold    \n",
       "19         1 Sanderson Square, Bromley, BR1 2FT  Last sold  - Dec 2007   \n",
       "20          1 Cavendish Place, Bromley, BR1 2GA  Last sold  - Mar 2006   \n",
       "21  Carrington, 1 Timms Close, Bromley, BR1 2GZ             Last sold    \n",
       "22            1 Mayfield Road, Bromley, BR1 2HB  Last sold  - Jun 2016   \n",
       "23            1 Lindsey Close, Bromley, BR1 2HE  Last sold  - Mar 2017   \n",
       "24            1 Birdham Close, Bromley, BR1 2HF             Last sold    \n",
       "25           1 Hawthorne Road, Bromley, BR1 2HG  Last sold  - Jul 2010   \n",
       "26          1 Hawthorne Close, Bromley, BR1 2HJ             Last sold    \n",
       "27               1 Coombe Lea, Bromley, BR1 2HQ             Last sold    \n",
       "28            1 Barfield Road, Bromley, BR1 2HR             Last sold    \n",
       "29        1 Wellsmoor Gardens, Bromley, BR1 2HT  Last sold  - Apr 1997   \n",
       "\n",
       "   last_sold_price          estimated_price  bedrooms  bathrooms  lounges  \\\n",
       "0          £97,500      £461,000 - £564,000       2.0        NaN      2.0   \n",
       "1         £675,000      £789,000 - £872,000       NaN        NaN      NaN   \n",
       "2         £480,000      £740,000 - £905,000       4.0        2.0      2.0   \n",
       "3         £420,000      £830,000 - £918,000       3.0        1.0      3.0   \n",
       "4         £376,000      £740,000 - £904,000       NaN        NaN      NaN   \n",
       "5         £185,000    £973,000 - £1,189,000       4.0        2.0      2.0   \n",
       "6         £375,000      £588,000 - £882,000       3.0        NaN      2.0   \n",
       "7         £571,000  £1,049,000 - £1,159,000       5.0        NaN      2.0   \n",
       "8                -      £506,000 - £619,000       NaN        NaN      NaN   \n",
       "9                -      £718,000 - £878,000       NaN        NaN      NaN   \n",
       "10        £219,000    £827,000 - £1,011,000       3.0        1.0      2.0   \n",
       "11               -      £522,000 - £638,000       3.0        1.0      1.0   \n",
       "12        £725,000  £1,251,000 - £1,529,000       NaN        NaN      NaN   \n",
       "13               -      £509,000 - £622,000       NaN        NaN      NaN   \n",
       "14      £1,135,000  £1,255,000 - £1,387,000       5.0        3.0      2.0   \n",
       "15        £325,000      £363,000 - £401,000       2.0        1.0      1.0   \n",
       "16        £485,000      £765,000 - £845,000       4.0        NaN      NaN   \n",
       "17               -      £640,000 - £960,000       NaN        NaN      NaN   \n",
       "18               -      £454,000 - £681,000       NaN        NaN      NaN   \n",
       "19        £745,000  £1,254,000 - £1,386,000       NaN        NaN      NaN   \n",
       "20        £850,000  £1,683,000 - £1,860,000       5.0        3.0      2.0   \n",
       "21               -  £1,478,000 - £2,217,000       NaN        NaN      NaN   \n",
       "22        £720,000    £849,000 - £1,038,000       4.0        1.0      2.0   \n",
       "23        £496,995      £573,000 - £633,000       3.0        1.0      NaN   \n",
       "24               -      £706,000 - £781,000       3.0        NaN      3.0   \n",
       "25      £1,425,000  £1,469,000 - £1,795,000       4.0        3.0      3.0   \n",
       "26               -  £1,183,000 - £1,775,000       NaN        NaN      NaN   \n",
       "27               -  £1,003,000 - £1,504,000       NaN        NaN      NaN   \n",
       "28               -      £887,000 - £980,000       NaN        NaN      NaN   \n",
       "29         £82,500      £515,000 - £629,000       NaN        NaN      NaN   \n",
       "\n",
       "                             url     tenure           property_type  \n",
       "0   /property/uprn/100020390465/   freehold     semi-detached house  \n",
       "1   /property/uprn/100020393770/   freehold          detached house  \n",
       "2   /property/uprn/100020408794/   freehold     semi-detached house  \n",
       "3   /property/uprn/100020390386/   freehold          detached house  \n",
       "4   /property/uprn/100020416127/   freehold          detached house  \n",
       "5   /property/uprn/100020415051/   freehold          detached house  \n",
       "6    /property/uprn/10003617154/   freehold       mid terrace house  \n",
       "7   /property/uprn/100020397356/   freehold          detached house  \n",
       "8   /property/uprn/100020409903/   freehold          detached house  \n",
       "9   /property/uprn/100020416607/        NaN          detached house  \n",
       "10  /property/uprn/100020392820/   freehold          detached house  \n",
       "11   /property/uprn/10070000055/  leasehold       end terrace house  \n",
       "12   /property/uprn/10070000046/   freehold       detached property  \n",
       "13   /property/uprn/10070000090/        NaN     semi-detached house  \n",
       "14   /property/uprn/10070000113/   freehold          detached house  \n",
       "15   /property/uprn/10070000302/   freehold         flat/maisonette  \n",
       "16   /property/uprn/10070000497/   freehold       end terrace house  \n",
       "17   /property/uprn/10070000521/   freehold       end terrace house  \n",
       "18   /property/uprn/10070000553/        NaN       end terrace house  \n",
       "19   /property/uprn/10070000569/   freehold          detached house  \n",
       "20   /property/uprn/10013152895/   freehold          detached house  \n",
       "21   /property/uprn/10013152686/   freehold          detached house  \n",
       "22  /property/uprn/100020406091/   freehold          detached house  \n",
       "23  /property/uprn/100020404698/   freehold       end terrace house  \n",
       "24  /property/uprn/100020390771/   freehold     semi-detached house  \n",
       "25  /property/uprn/100020399817/   freehold          detached house  \n",
       "26  /property/uprn/100020399810/   freehold          detached house  \n",
       "27  /property/uprn/100020395001/   freehold          detached house  \n",
       "28  /property/uprn/100020389621/        NaN       detached bungalow  \n",
       "29  /property/uprn/100020418834/   freehold  semi-detached property  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd3bca-658a-44f9-a58d-60477c8aee7c",
   "metadata": {},
   "source": [
    "## CREATING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319988de-17f6-4e43-8562-ea3d2da3f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSV files list from a folder\n",
    "path = os.getcwd()\n",
    "csv_files = glob.glob(path + \"data/csv_data/*.csv\")\n",
    "\n",
    "#Could have separated these during the scrape but we separate the files by which country they are in...\n",
    "#This will help when we are tidying up our salary data and adding city columns\n",
    "usa_csv_files = [file_name for file_name in csv_files for city in usa_cities if \"/{}_scrape.csv\".format(city.replace(\" \", \"_\")) in file_name]\n",
    "uk_csv_files = [file_name for file_name in csv_files for city in uk_cities if \"/{}_scrape.csv\".format(city.replace(\" \", \"_\")) in file_name]\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "\n",
    "df_list = []\n",
    "for file in usa_csv_files:\n",
    "    current_df = pd.read_csv(file, sep='\\t')\n",
    "    current_city = [city for city in usa_cities if \"/{}_scrape.csv\".format(city.replace(\" \", \"_\")) in file][0]\n",
    "    current_df['city'] = current_city\n",
    "    current_df['country'] = 'usa'\n",
    "    df_list.append(current_df)\n",
    "    \n",
    "for file in uk_csv_files:\n",
    "    current_df = pd.read_csv(file, sep='\\t')\n",
    "    current_city = [city for city in uk_cities if \"/{}_scrape.csv\".format(city.replace(\" \", \"_\")) in file][0]\n",
    "    current_df['city'] = current_city\n",
    "    current_df['country'] = 'uk'\n",
    "    df_list.append(current_df)\n",
    "    \n",
    "\n",
    "#df_list = (pd.read_csv(file, sep='\\t') for file in csv_files)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
